{
  
    
        "post0": {
            "title": "Kubeflow 1.1 improves ML Workflow Productivity, Isolation & Security, and GitOps",
            "content": "The Kubeflow Community’s delivery of Kubeflow 1.1 offers users valuable ML workflow automation with Fairing and Kale along with MXNet, MPI and XGBoost distributed training operators. It extends isolation and security through the delivery of multi-user pipelines, CVE scanning, and support for Google’s Private GKE and Anthos. 1.1 provides a foundation for consistent and repeatable installation and operations using GitOps methodologies powered by blueprints and kpt primitives. . The ML productivity enhancements in 1.1 include end-to-end workflows using Fairing and Kale. The Fairing workflow enables users to build, train and deploy models from a notebook and Fairing improvements include the support of configuring environment variables and mounting secrets. Fairing also added a config map for a deployer and bug fixes for TensorRTSpec. The workflows enabled by Kale include the ability to write model code in a notebook and then automatically build a Kubeflow pipeline that deploys, trains and tunes that model efficiently, using Katib and cached pipeline steps. Kubeflow 1.1 also delivers stable release deliveries of MXNet, MPI and XGBoost operators, which simplify distributed training on multiple nodes and speeds model creation. . The isolation and security feature deliveries include Private GKE and Anthos support, a stable version of Kubeflow Pipelines with Multi-User Kubeflow Pipelines support, and a process for Kubeflow container image scanning, CVE reporting, and an optional process for distroless image creation. 1.1 also includes options for authentication and authorization. This includes the option for administrators to turn off self-service namespace creation mode, as admins may have other processes for namespace creation. The Community also developed a best practice to build user authorization in Kubeflow web apps using subject access review. . The installation and operations of Kubeflow have been enhanced to support GitOps methodologies. Several Kubeflow platform providers and software support vendors are developing time-saving GitOps processes to simplify and codify the installation, configuration and operations of the various layers in the Kubeflow 1.1 hardware and software stack. Some examples are provided in the next section. . More details and 1.1 tutorials . Kubeflow 1.1 includes many technical enhancements, which are being delivered via the Community’s release process. Details on the application feature development can be found in the 1.1 KanBan Board and in the Kubeflow Roadmap. As the Kubeflow application improvements are merged, the platform teams (GCP, AWS, IBM, Red Hat, Azure, and Arrikto MiniKF) are working to validate the feature improvements on their respective environments. . Kubeflow 1.1 includes KFServing v0.3, where the focus has been on providing more stability by doing a major move to KNative v1 APIs. Additionally, we added GPU support for PyTorch model servers, and pickled model format support for SKLearn. There were other enhancements vis a vis routing, payload logging, bug fixes etc., details of which can be found here. . Kubeflow 1.1 demo scripts and workflow tutorials are available as validated by the individual platforms. Please find those below: . Kubeflow 1.1 Tutorial for Automated Hyperparameter Tuning, Multi-user Pipelines, Pipeline Caching | GitOps for Kubeflow 1.1 on AWS EKS | . 1.1 users can also leverage several other Kubeflow ecosystem tools including: . Seldon Core 1.1, which handles scaling to thousands of production ML models and provides advanced ML capabilities including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more. | Feast: A feature store that allows teams ML teams to define, manage, discover, and serve ML features to their models. | . What’s Coming and Getting involved . The Kubeflow Community has started planning for its next release. Although we have a nice backlog of issues, our process includes discussions and surveys with users and contributors to validate use cases and their value. . The Community continues to refine its governance and refine this proposal, Proposal for Kubeflow WG Guidelines/Governance. We are actively developing Working Group team charters, tech leads, chairs and members. We look forward to this growth. . The following provides some helpful links to those looking to get involved with the Kubeflow Community: . Join the Kubeflow Slack channel | Join the kubeflow-discuss mailing list | Attend a weekly community meeting | . If you have questions, run into issues, please leverage the Slack channel and/or submit bugs via Kubeflow on GitHub. Thanks from all of us in the Community, and we look forward to your success with Kubeflow 1.1. . Special thanks to Yuan Tang (Ant Group), Dan Sun (Bloomberg), Josh Bottum (Arrikto), Constantinos Venetsanopoulos (Arrikto), Yannis Zarkadas (Arrikto), Hamel Husain (GitHub), Willem Pienaar (GoJek), Yuan Gong (Google), Jeremy Lewi (Google), Animesh Singh (IBM) and Clive Cox (Seldon) for their help on this post. .",
            "url": "https://blog.kubeflow.org/release/official/2020/07/31/kubeflow-1.1-blog-post.html",
            "relUrl": "/release/official/2020/07/31/kubeflow-1.1-blog-post.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Train, deploy and record metadata on Kubeflow from Notebooks",
            "content": "Train and deploy on Kubeflow from Notebooks . This notebook shows you how to use Kubeflow to build, train, and deploy models on Kubernetes. This notebook walks you through the following . Building an XGBoost model inside a notebook | Training the model inside the notebook | Performing inference using the model inside the notebook | Using Kubeflow Fairing to launch training jobs on Kubernetes | Using Kubeflow Fairing to build and deploy a model using Seldon Core | Using Kubeflow metadata to record metadata about your models | Using Kubeflow Pipelines to build a pipeline to train your model | . Prerequisites . This notebook assumes you are running inside 0.6 Kubeflow deployed on GKE following the GKE instructions | If you are running somewhere other than GKE you will need to modify the notebook to use a different docker registry or else configure Kubeflow to work with GCR. | . Verify we have a GCP account . The cell below checks that this notebook was spawned with credentials to access GCP | . import os from oauth2client.client import GoogleCredentials credentials = GoogleCredentials.get_application_default() . Install Required Libraries . Import the libraries required to train this model. . import notebook_setup notebook_setup.notebook_setup() . pip installing requirements.txt pip installing KFP https://storage.googleapis.com/ml-pipeline/release/0.1.32/kfp.tar.gz pip installing fairing git+git://github.com/kubeflow/fairing.git@9b0d4ed4796ba349ac6067bbd802ff1d6454d015 Configure docker credentials . Import the python libraries we will use | We add a comment &quot;fairing:include-cell&quot; to tell the kubefow fairing preprocessor to keep this cell when converting to python code later | . # fairing:include-cell import fire import joblib import logging import nbconvert import os import pathlib import sys from pathlib import Path import pandas as pd import pprint from sklearn.metrics import mean_absolute_error from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from xgboost import XGBRegressor from importlib import reload from sklearn.datasets import make_regression from kubeflow.metadata import metadata from datetime import datetime import retrying import urllib3 . # Imports not to be included in the built docker image import util import kfp import kfp.components as comp import kfp.gcp as gcp import kfp.dsl as dsl import kfp.compiler as compiler from kubernetes import client as k8s_client from kubeflow import fairing from kubeflow.fairing.builders import append from kubeflow.fairing.deployers import job from kubeflow.fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire . Code to train and predict . In the cells below we define some functions to generate data and train a model | These functions could just as easily be defined in a separate python module | . # fairing:include-cell def read_synthetic_input(test_size=0.25): &quot;&quot;&quot;generate synthetic data and split it into train and test.&quot;&quot;&quot; # generate regression dataset X, y = make_regression(n_samples=200, n_features=5, noise=0.1) train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size, shuffle=False) imputer = SimpleImputer() train_X = imputer.fit_transform(train_X) test_X = imputer.transform(test_X) return (train_X, train_y), (test_X, test_y) . # fairing:include-cell def train_model(train_X, train_y, test_X, test_y, n_estimators, learning_rate): &quot;&quot;&quot;Train the model using XGBRegressor.&quot;&quot;&quot; model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate) model.fit(train_X, train_y, early_stopping_rounds=40, eval_set=[(test_X, test_y)]) print(&quot;Best RMSE on eval: %.2f with %d rounds&quot;, model.best_score, model.best_iteration+1) return model def eval_model(model, test_X, test_y): &quot;&quot;&quot;Evaluate the model performance.&quot;&quot;&quot; predictions = model.predict(test_X) mae=mean_absolute_error(predictions, test_y) logging.info(&quot;mean_absolute_error=%.2f&quot;, mae) return mae def save_model(model, model_file): &quot;&quot;&quot;Save XGBoost model for serving.&quot;&quot;&quot; joblib.dump(model, model_file) logging.info(&quot;Model export success: %s&quot;, model_file) def create_workspace(): METADATA_STORE_HOST = &quot;metadata-grpc-service.kubeflow&quot; # default DNS of Kubeflow Metadata gRPC serivce. METADATA_STORE_PORT = 8080 return metadata.Workspace( store=metadata.Store(grpc_host=METADATA_STORE_HOST, grpc_port=METADATA_STORE_PORT), name=&quot;xgboost-synthetic&quot;, description=&quot;workspace for xgboost-synthetic artifacts and executions&quot;) . Wrap Training and Prediction in a class . In the cell below we wrap training and prediction in a class | A class provides the structure we will need to eventually use kubeflow fairing to launch separate training jobs and/or deploy the model on Kubernetes | . # fairing:include-cell class ModelServe(object): def __init__(self, model_file=None): self.n_estimators = 50 self.learning_rate = 0.1 if not model_file: if &quot;MODEL_FILE&quot; in os.environ: print(&quot;model_file not supplied; checking environment variable&quot;) model_file = os.getenv(&quot;MODEL_FILE&quot;) else: print(&quot;model_file not supplied; using the default&quot;) model_file = &quot;mockup-model.dat&quot; self.model_file = model_file print(&quot;model_file={0}&quot;.format(self.model_file)) self.model = None self._workspace = None self.exec = self.create_execution() def train(self): (train_X, train_y), (test_X, test_y) = read_synthetic_input() # Here we use Kubeflow&#39;s metadata library to record information # about the training run to Kubeflow&#39;s metadata store. self.exec.log_input(metadata.DataSet( description=&quot;xgboost synthetic data&quot;, name=&quot;synthetic-data&quot;, owner=&quot;someone@kubeflow.org&quot;, uri=&quot;file://path/to/dataset&quot;, version=&quot;v1.0.0&quot;)) model = train_model(train_X, train_y, test_X, test_y, self.n_estimators, self.learning_rate) mae = eval_model(model, test_X, test_y) # Here we log metrics about the model to Kubeflow&#39;s metadata store. self.exec.log_output(metadata.Metrics( name=&quot;xgboost-synthetic-traing-eval&quot;, owner=&quot;someone@kubeflow.org&quot;, description=&quot;training evaluation for xgboost synthetic&quot;, uri=&quot;gcs://path/to/metrics&quot;, metrics_type=metadata.Metrics.VALIDATION, values={&quot;mean_absolute_error&quot;: mae})) save_model(model, self.model_file) self.exec.log_output(metadata.Model( name=&quot;housing-price-model&quot;, description=&quot;housing price prediction model using synthetic data&quot;, owner=&quot;someone@kubeflow.org&quot;, uri=self.model_file, model_type=&quot;linear_regression&quot;, training_framework={ &quot;name&quot;: &quot;xgboost&quot;, &quot;version&quot;: &quot;0.9.0&quot; }, hyperparameters={ &quot;learning_rate&quot;: self.learning_rate, &quot;n_estimators&quot;: self.n_estimators }, version=datetime.utcnow().isoformat(&quot;T&quot;))) def predict(self, X, feature_names): &quot;&quot;&quot;Predict using the model for given ndarray. The predict signature should match the syntax expected by Seldon Core https://github.com/SeldonIO/seldon-core so that we can use Seldon h to wrap it a model server and deploy it on Kubernetes &quot;&quot;&quot; if not self.model: self.model = joblib.load(self.model_file) # Do any preprocessing prediction = self.model.predict(data=X) # Do any postprocessing return [[prediction.item(0), prediction.item(1)]] @property def workspace(self): if not self._workspace: self._workspace = create_workspace() return self._workspace def create_execution(self): r = metadata.Run( workspace=self.workspace, name=&quot;xgboost-synthetic-faring-run&quot; + datetime.utcnow().isoformat(&quot;T&quot;), description=&quot;a notebook run&quot;) return metadata.Execution( name = &quot;execution&quot; + datetime.utcnow().isoformat(&quot;T&quot;), workspace=self.workspace, run=r, description=&quot;execution for training xgboost-synthetic&quot;) . Train your Model Locally . Train your model locally inside your notebook | To train locally we just instatiante the ModelServe class and then call train | . model = ModelServe(model_file=&quot;mockup-model.dat&quot;) model.train() . MetadataStore with gRPC connection initialized . model_file=mockup-model.dat [23:44:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [0] validation_0-rmse:134.005 Will train until validation_0-rmse hasn&#39;t improved in 40 rounds. [1] validation_0-rmse:129.102 [2] validation_0-rmse:124.39 [3] validation_0-rmse:119.218 [4] validation_0-rmse:114.096 [5] validation_0-rmse:109.494 [6] validation_0-rmse:107.101 [7] validation_0-rmse:103.463 [8] validation_0-rmse:100.657 [9] validation_0-rmse:96.576 [10] validation_0-rmse:94.8884 [11] validation_0-rmse:91.7095 [12] validation_0-rmse:90.7389 [13] validation_0-rmse:88.1934 [14] validation_0-rmse:86.1535 [15] validation_0-rmse:84.8222 [16] validation_0-rmse:83.5818 [17] validation_0-rmse:81.6697 [18] validation_0-rmse:80.2789 [19] validation_0-rmse:79.4583 [20] validation_0-rmse:78.4213 [21] validation_0-rmse:77.0478 [22] validation_0-rmse:75.3792 [23] validation_0-rmse:73.9913 [24] validation_0-rmse:73.2026 [25] validation_0-rmse:72.2079 [26] validation_0-rmse:70.9489 [27] validation_0-rmse:70.5206 [28] validation_0-rmse:69.8641 [29] validation_0-rmse:69.0409 [30] validation_0-rmse:68.3776 [31] validation_0-rmse:67.2776 [32] validation_0-rmse:66.7612 [33] validation_0-rmse:65.9548 [34] validation_0-rmse:65.5048 [35] validation_0-rmse:64.8582 [36] validation_0-rmse:64.118 [37] validation_0-rmse:63.5615 [38] validation_0-rmse:63.2716 [39] validation_0-rmse:62.9765 [40] validation_0-rmse:62.3468 [41] validation_0-rmse:62.0579 [42] validation_0-rmse:61.9598 [43] validation_0-rmse:61.6452 [44] validation_0-rmse:61.2468 [45] validation_0-rmse:60.7332 [46] validation_0-rmse:60.6493 [47] validation_0-rmse:60.2032 [48] validation_0-rmse:59.9972 [49] validation_0-rmse:59.5956 . mean_absolute_error=47.22 Model export success: mockup-model.dat . Best RMSE on eval: %.2f with %d rounds 59.595573 50 . Predict locally . Run prediction inside the notebook using the newly created model | To run prediction we just invoke redict | . (train_X, train_y), (test_X, test_y) =read_synthetic_input() ModelServe().predict(test_X, None) . MetadataStore with gRPC connection initialized . model_file not supplied; using the default model_file=mockup-model.dat [23:44:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . [[-30.6968994140625, 45.884098052978516]] . Use Kubeflow Fairing to Launch a K8s Job to train your model . Now that we have trained a model locally we can use Kubeflow fairing to Launch a Kubernetes job to train the model | Deploy the model on Kubernetes | | Launching a separate Kubernetes job to train the model has the following advantages . You can leverage Kubernetes to run multiple training jobs in parallel | You can run long running jobs without blocking your kernel | . | . Configure The Docker Registry For Kubeflow Fairing . In order to build docker images from your notebook we need a docker registry where the images will be stored | Below you set some variables specifying a GCR container registry | Kubeflow Fairing provides a utility function to guess the name of your GCP project | . # Setting up google container repositories (GCR) for storing output containers # You can use any docker container registry istead of GCR GCP_PROJECT = fairing.cloud.gcp.guess_project_name() DOCKER_REGISTRY = &#39;gcr.io/{}/fairing-job&#39;.format(GCP_PROJECT) . Use Kubeflow fairing to build the docker image . First you will use kubeflow fairing&#39;s kaniko builder to build a docker image that includes all your dependencies You use kaniko because you want to be able to run pip to install dependencies | Kaniko gives you the flexibility to build images from Dockerfiles | . | kaniko, however, can be slow | so you will build a base image using Kaniko and then every time your code changes you will just build an image starting from your base image and adding your code to it | you use the kubeflow fairing build to enable these fast rebuilds | . # TODO(https://github.com/kubeflow/fairing/issues/426): We should get rid of this once the default # Kaniko image is updated to a newer image than 0.7.0. from kubeflow.fairing import constants constants.constants.KANIKO_IMAGE = &quot;gcr.io/kaniko-project/executor:v0.14.0&quot; . from kubeflow.fairing.builders import cluster # output_map is a map of extra files to add to the notebook. # It is a map from source location to the location inside the context. output_map = { &quot;Dockerfile&quot;: &quot;Dockerfile&quot;, &quot;requirements.txt&quot;: &quot;requirements.txt&quot;, } preprocessor = ConvertNotebookPreprocessorWithFire(class_name=&#39;ModelServe&#39;, notebook_file=&#39;build-train-deploy.ipynb&#39;, output_map=output_map) if not preprocessor.input_files: preprocessor.input_files = set() input_files=[&quot;xgboost_util.py&quot;, &quot;mockup-model.dat&quot;] preprocessor.input_files = set([os.path.normpath(f) for f in input_files]) preprocessor.preprocess() . Converting build-train-deploy.ipynb to build-train-deploy.py Creating entry point for the class name ModelServe . [PosixPath(&#39;build-train-deploy.py&#39;), &#39;xgboost_util.py&#39;, &#39;mockup-model.dat&#39;] . Build the base image . You use cluster_builder to build the base image | You only need to perform this again if we change our Docker image or the dependencies we need to install | ClusterBuilder takes as input the DockerImage to use as a base image | You should use the same Jupyter image that you are using for your notebook server so that your environment will be the same when you launch Kubernetes jobs | . # Use a stock jupyter image as our base image # TODO(jlewi): Should we try to use the downward API to default to the image we are running in? base_image = &quot;gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0&quot; # We use a custom Dockerfile cluster_builder = cluster.cluster.ClusterBuilder(registry=DOCKER_REGISTRY, base_image=base_image, preprocessor=preprocessor, dockerfile_path=&quot;Dockerfile&quot;, pod_spec_mutators=[fairing.cloud.gcp.add_gcp_credentials_if_exists], context_source=cluster.gcs_context.GCSContextSource()) cluster_builder.build() . Building image using cluster builder. Creating docker context: /tmp/fairing_context_n34sz0lr Converting build-train-deploy.ipynb to build-train-deploy.py Creating entry point for the class name ModelServe Not able to find gcp credentials secret: user-gcp-sa Trying workload identity service account: default-editor Waiting for fairing-builder-dcbz2-lqzjg to start... Waiting for fairing-builder-dcbz2-lqzjg to start... Waiting for fairing-builder-dcbz2-lqzjg to start... Pod started running True . ERROR: logging before flag.Parse: E0226 23:44:52.505936 1 metadata.go:241] Failed to unmarshal scopes: invalid character &#39;h&#39; looking for beginning of value INFO[0002] Resolved base name gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 to gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0002] Resolved base name gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 to gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0002] Downloading base image gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0002] Error while retrieving image from cache: getting file info: stat /cache/sha256:fe174faf7c477bc3dae796b067d98ac3f0d31e8075007a1146f86d13f2c98e13: no such file or directory INFO[0002] Downloading base image gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0003] Built cross stage deps: map[] INFO[0003] Downloading base image gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0003] Error while retrieving image from cache: getting file info: stat /cache/sha256:fe174faf7c477bc3dae796b067d98ac3f0d31e8075007a1146f86d13f2c98e13: no such file or directory INFO[0003] Downloading base image gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0 INFO[0003] Using files from context: [/kaniko/buildcontext/requirements.txt] INFO[0003] Checking for cached layer gcr.io/kubeflow-ci/fairing-job/fairing-job/cache:233bc2f24de09b29aa4c12d0f5adcc3098286c3c35eb0b4864fa00f73d8b9d2c... INFO[0003] Using caching version of cmd: COPY requirements.txt . INFO[0003] cmd: USER INFO[0003] Checking for cached layer gcr.io/kubeflow-ci/fairing-job/fairing-job/cache:1acac4c9cb73d1b18003ae6076fd264e37af3983927234122784b04452b9b44e... INFO[0004] Using caching version of cmd: RUN pip3 --no-cache-dir install -r requirements.txt INFO[0004] cmd: USER INFO[0004] Skipping unpacking as no commands require it. INFO[0004] Taking snapshot of full filesystem... INFO[0004] COPY requirements.txt . INFO[0004] Found cached layer, extracting to filesystem INFO[0004] extractedFiles: [/tf/requirements.txt / /tf] INFO[0004] Taking snapshot of files... INFO[0004] USER root INFO[0004] cmd: USER INFO[0004] No files changed in this command, skipping snapshotting. INFO[0004] RUN pip3 --no-cache-dir install -r requirements.txt INFO[0004] Found cached layer, extracting to filesystem INFO[0032] Taking snapshot of files... INFO[0070] USER jovyan INFO[0070] cmd: USER INFO[0070] No files changed in this command, skipping snapshotting. . Build the actual image . Here you use the append builder to add your code to the base image . Calling preprocessor.preprocess() converts your notebook file to a python file . You are using the ConvertNotebookPreprocessorWithFire | This preprocessor converts ipynb files to py files by doing the following . Removing all cells which don&#39;t have a comment # fairing:include-cell | Using python-fire to add entry points for the class specified in the constructor | | Call preprocess() will create the file build-train-deploy.py . | . | You use the AppendBuilder to rapidly build a new docker image by quickly adding some files to an existing docker image . The AppendBuilder is super fast so its very convenient for rebuilding your images as you iterate on your code | The AppendBuilder will add the converted notebook, build-train-deploy.py, along with any files specified in preprocessor.input_files to /app in the newly created image | . | . preprocessor.preprocess() builder = append.append.AppendBuilder(registry=DOCKER_REGISTRY, base_image=cluster_builder.image_tag, preprocessor=preprocessor) builder.build() . Converting build-train-deploy.ipynb to build-train-deploy.py Creating entry point for the class name ModelServe Building image using Append builder... Creating docker context: /tmp/fairing_context_x4g0orab Converting build-train-deploy.ipynb to build-train-deploy.py Creating entry point for the class name ModelServe build-train-deploy.py already exists in Fairing context, skipping... Loading Docker credentials for repository &#39;gcr.io/kubeflow-ci/fairing-job/fairing-job:F47EE88D&#39; Invoking &#39;docker-credential-gcloud&#39; to obtain Docker credentials. Successfully obtained Docker credentials. Image successfully built in 2.249176573008299s. Pushing image gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77... Loading Docker credentials for repository &#39;gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77&#39; Invoking &#39;docker-credential-gcloud&#39; to obtain Docker credentials. Successfully obtained Docker credentials. Uploading gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77 Layer sha256:8832e37735788665026956430021c6d1919980288c66c4526502965aeb5ac006 exists, skipping Layer sha256:b4ecb6928817c974946ba93ffc5ce60de886457eb57955dae9d7bc8facfb690a exists, skipping Layer sha256:9269cef1ab8b202433fe1dfbfbdf4649926d70d7a8b94f0324421bda79b917fa exists, skipping Layer sha256:d77b634303a107b22366d05d1071bf79e7d2a27b3d6db1fb726fcfd5dd5f9831 exists, skipping Layer sha256:5bd1cb59702536c10e96bb14e54846922c9b257580d4e2c733076a922525240b exists, skipping Layer sha256:7babe47a4c402afbe26f10dffceb85be7bfd2072a96b816814503f41ce9c5273 exists, skipping Layer sha256:21a7832aeb8625dc8228ceb115a28222f87e0fbce61b2588c42a2cce7a3a63d6 exists, skipping Layer sha256:107cba84ef3d72ed995c76c7a4f60ba5613f58b029ab7e42ac20ece99bec88b1 exists, skipping Layer sha256:a31c3b1caad473a474d574283741f880e37c708cc06ee620d3e93fa602125ee0 exists, skipping Layer sha256:92d24c89f5bc70958385728755b042a5a45bddf2f997de80e84d1161f43ba316 exists, skipping Layer sha256:e590ee7edf442435692956d6fed54190416a217147a50c63e73a6a78d15bec84 exists, skipping Layer sha256:96685dce34a0d24bf69741972441398cffbed89aed4f40e3c063176c59a3c81c exists, skipping Layer sha256:daa5c419d33d51d1730ea530f4f7335640f5bb42856f319c63a1a521aee368c1 exists, skipping Layer sha256:016724bbd2c9643f24eff7c1e86d9202d7c04caddd7fdd4375a77e3998ce8203 exists, skipping Layer sha256:b5494e32d0131350be270a54399cee65934e90d3c2df87a83757903e627813b2 exists, skipping Layer sha256:823f4685c03b26a545ca41dcdca1e782ad5e52cf85bac03113edaa6aebdca1b3 exists, skipping Layer sha256:777cec03b3e23c21f8cf78f07812cc83dd7f352719226f27f361c5b706f6a93f exists, skipping Layer sha256:dc2840b4417186d66a29d64a039ac164be95929211d808294d36acae9301fc6b exists, skipping Layer sha256:5e671b828b2af02924968841e5d12084fa78e8722e9510402aaee80dc5d7a6db exists, skipping Layer sha256:5bac0c144f6e0b7082e3691da95d3f057ee0be0735e9efca76096da59cfd1786 exists, skipping Layer sha256:5b7339215d1d5f8e68622d584a224f60339f5bef41dbd74330d081e912f0cddd exists, skipping Layer sha256:35daced67e5901b8de4a92bca9fdc67c8593d400aae483591987442f54c87d0a exists, skipping Layer sha256:330a9002e0b4aa1e27d3628dd3f02ff9a39d25745b8f2f219b06e3725153ffc0 exists, skipping Layer sha256:4e8a6b90828e0d339f646d723df8720ffa17c0ffb905f8f009faf1be320ab5d9 exists, skipping Layer sha256:2b940936f9933b7737cf407f2149dd7393998d7a0bee5acf1c4a57b0487cef79 exists, skipping Layer sha256:d684674aa1a4d080be26286fd9356f573b80d2448599392e3dcf3c61ce98a0f0 exists, skipping Layer sha256:68543864d6442a851eaff0500161b92e4a151051cf7ed2649b3790a3f876bada exists, skipping Layer sha256:21640f54008ccbfc0d100246633f8e6f18f918a0566561f61aebbda785321e56 exists, skipping Layer sha256:f44c204b040238da05a21af1fd8543ea95f1e9249fac34b3b65217e38815568d exists, skipping Layer sha256:b054a26005b7f3b032577f811421fab5ec3b42ce45a4012dfa00cf6ed6191b0f exists, skipping Layer sha256:14ca88e9f6723ce82bc14b241cda8634f6d19677184691d086662641ab96fe68 exists, skipping Layer sha256:e3ab47ad84d9e11c5fad45791ce00ec5b5f3b7f1ae61a5fab17eb44c399d910f exists, skipping Layer sha256:01ad04a655b291ed8502f23f5b8c73d94475763e9b3cdbf6d1107f7879aadac6 exists, skipping Layer sha256:4b9d9f2fa2a2b168f0a49fcd3074c885ab1ca2c507848f7b2e3cee8104f1f7c3 exists, skipping Layer sha256:5bd2e6f0de430cd3936eec59afb6cf466b052344fe4348ac33a48ac903b661e2 exists, skipping Layer sha256:15bca5bd6fdc1b1ac156de08ce3b0f57760b345556b64017d1be5cc7c95e5e5b pushed. Layer sha256:d23f2bdcc84f066126b083288f75d140d58fc252618bda5bf05cb9696a183958 pushed. Finished upload of: gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77 Pushed image gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77 in 2.74867532402277s. . Launch the K8s Job . You can use kubeflow fairing to easily launch a Kubernetes job to invoke code | You use fairings Kubernetes job library to build a Kubernetes job You use pod mutators to attach GCP credentials to the pod | You can also use pod mutators to attch PVCs | . | Since the ConvertNotebookPreprocessorWithFire is using python-fire you can easily invoke any method inside the ModelServe class just by configuring the command invoked by the Kubernetes job In the cell below you extend the command to include train as an argument because you want to invoke the train function | . | . Note When you invoke train_deployer.deploy; kubeflow fairing will stream the logs from the Kubernetes job. The job will initially show some connection errors because the job will try to connect to the metadataserver. You can ignore these errors; the job will retry until its able to connect and then continue . pod_spec = builder.generate_pod_spec() train_deployer = job.job.Job(cleanup=False, pod_spec_mutators=[ fairing.cloud.gcp.add_gcp_credentials_if_exists]) # Add command line arguments pod_spec.containers[0].command.extend([&quot;train&quot;]) result = train_deployer.deploy(pod_spec) . Not able to find gcp credentials secret: user-gcp-sa Trying workload identity service account: default-editor The job fairing-job-qwdlb launched. Waiting for fairing-job-qwdlb-67ddb to start... Waiting for fairing-job-qwdlb-67ddb to start... Waiting for fairing-job-qwdlb-67ddb to start... Pod started running True . 2020-02-26 23:48:04.056153: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer.so.6&#39;; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory 2020-02-26 23:48:04.056318: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer_plugin.so.6&#39;; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory 2020-02-26 23:48:04.056332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. WARNING: Logging before flag parsing goes to stderr. I0226 23:48:06.277673 140238089848640 metadata_store.py:80] MetadataStore with gRPC connection initialized model_file not supplied; using the default model_file=mockup-model.dat [23:48:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [0] validation_0-rmse:106.201 Will train until validation_0-rmse hasn&#39;t improved in 40 rounds. [1] validation_0-rmse:102.289 [2] validation_0-rmse:99.0904 [3] validation_0-rmse:95.5223 [4] validation_0-rmse:92.2357 [5] validation_0-rmse:90.1649 [6] validation_0-rmse:87.6004 [7] validation_0-rmse:85.4127 [8] validation_0-rmse:82.7163 [9] validation_0-rmse:81.1641 [10] validation_0-rmse:79.1006 [11] validation_0-rmse:77.2564 [12] validation_0-rmse:75.3755 [13] validation_0-rmse:74.3393 [14] validation_0-rmse:72.0505 [15] validation_0-rmse:70.8315 [16] validation_0-rmse:69.1124 [17] validation_0-rmse:67.9681 [18] validation_0-rmse:66.2094 [19] validation_0-rmse:64.6999 [20] validation_0-rmse:63.6925 [21] validation_0-rmse:62.261 [22] validation_0-rmse:60.887 [23] validation_0-rmse:59.5543 [24] validation_0-rmse:58.3673 [25] validation_0-rmse:57.0439 [26] validation_0-rmse:55.7172 [27] validation_0-rmse:54.7011 [28] validation_0-rmse:53.8976 [29] validation_0-rmse:53.3325 [30] validation_0-rmse:52.81 [31] validation_0-rmse:51.8806 [32] validation_0-rmse:50.9026 [33] validation_0-rmse:50.0451 [34] validation_0-rmse:49.2711 [35] validation_0-rmse:48.6533 [36] validation_0-rmse:47.8613 [37] validation_0-rmse:47.5519 [38] validation_0-rmse:46.9383 [39] validation_0-rmse:46.7275 [40] validation_0-rmse:46.1317 [41] validation_0-rmse:45.7704 [42] validation_0-rmse:45.4888 [43] validation_0-rmse:44.8847 [44] validation_0-rmse:44.5583 [45] validation_0-rmse:43.9202 [46] validation_0-rmse:43.7332 [47] validation_0-rmse:43.2122 [48] validation_0-rmse:43.0383 [49] validation_0-rmse:42.7427 I0226 23:48:06.457567 140238089848640 build-train-deploy.py:100] mean_absolute_error=33.15 I0226 23:48:06.494030 140238089848640 build-train-deploy.py:106] Model export success: mockup-model.dat Best RMSE on eval: %.2f with %d rounds 42.742691 50 . You can use kubectl to inspect the job that fairing created | . !kubectl get jobs -l fairing-id={train_deployer.job_id} -o yaml . apiVersion: v1 items: - apiVersion: batch/v1 kind: Job metadata: creationTimestamp: &#34;2020-02-26T23:47:21Z&#34; generateName: fairing-job- labels: fairing-deployer: job fairing-id: 54d568cc-58f2-11ea-964d-46fd3ccc57c5 name: fairing-job-qwdlb namespace: zhenghui resourceVersion: &#34;11375571&#34; selfLink: /apis/batch/v1/namespaces/zhenghui/jobs/fairing-job-qwdlb uid: 54d8d81b-58f2-11ea-a99d-42010a8000ac spec: backoffLimit: 0 completions: 1 parallelism: 1 selector: matchLabels: controller-uid: 54d8d81b-58f2-11ea-a99d-42010a8000ac template: metadata: annotations: sidecar.istio.io/inject: &#34;false&#34; creationTimestamp: null labels: controller-uid: 54d8d81b-58f2-11ea-a99d-42010a8000ac fairing-deployer: job fairing-id: 54d568cc-58f2-11ea-964d-46fd3ccc57c5 job-name: fairing-job-qwdlb name: fairing-deployer spec: containers: - command: - python - /app/build-train-deploy.py - train env: - name: FAIRING_RUNTIME value: &#34;1&#34; image: gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77 imagePullPolicy: IfNotPresent name: fairing-job resources: {} securityContext: runAsUser: 0 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File workingDir: /app/ dnsPolicy: ClusterFirst restartPolicy: Never schedulerName: default-scheduler securityContext: {} serviceAccount: default-editor serviceAccountName: default-editor terminationGracePeriodSeconds: 30 status: completionTime: &#34;2020-02-26T23:48:08Z&#34; conditions: - lastProbeTime: &#34;2020-02-26T23:48:08Z&#34; lastTransitionTime: &#34;2020-02-26T23:48:08Z&#34; status: &#34;True&#34; type: Complete startTime: &#34;2020-02-26T23:47:21Z&#34; succeeded: 1 kind: List metadata: resourceVersion: &#34;&#34; selfLink: &#34;&#34; . Deploy the trained model to Kubeflow for predictions . Now that you have trained a model you can use kubeflow fairing to deploy it on Kubernetes | When you call deployer.deploy fairing will create a Kubernetes Deployment to serve your model | Kubeflow fairing uses the docker image you created earlier | The docker image you created contains your code and Seldon core | Kubeflow fairing uses Seldon to wrap your prediction code, ModelServe.predict, in a REST and gRPC server | . from kubeflow.fairing.deployers import serving pod_spec = builder.generate_pod_spec() module_name = os.path.splitext(preprocessor.executable.name)[0] deployer = serving.serving.Serving(module_name + &quot;.ModelServe&quot;, service_type=&quot;ClusterIP&quot;, labels={&quot;app&quot;: &quot;mockup&quot;}) url = deployer.deploy(pod_spec) . Cluster endpoint: http://fairing-service-kkbtm.zhenghui.svc.cluster.local:5000/predict . You can use kubectl to inspect the deployment that fairing created | . !kubectl get deploy -o yaml {deployer.deployment.metadata.name} . apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: &#34;1&#34; creationTimestamp: &#34;2020-02-26T23:48:12Z&#34; generateName: fairing-deployer- generation: 1 labels: app: mockup fairing-deployer: serving fairing-id: 73532514-58f2-11ea-964d-46fd3ccc57c5 name: fairing-deployer-p8xc9 namespace: zhenghui resourceVersion: &#34;11375642&#34; selfLink: /apis/extensions/v1beta1/namespaces/zhenghui/deployments/fairing-deployer-p8xc9 uid: 7354b5ec-58f2-11ea-a99d-42010a8000ac spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: mockup fairing-deployer: serving fairing-id: 73532514-58f2-11ea-964d-46fd3ccc57c5 strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: annotations: sidecar.istio.io/inject: &#34;false&#34; creationTimestamp: null labels: app: mockup fairing-deployer: serving fairing-id: 73532514-58f2-11ea-964d-46fd3ccc57c5 name: fairing-deployer spec: containers: - command: - seldon-core-microservice - build-train-deploy.ModelServe - REST - --service-type=MODEL - --persistence=0 env: - name: FAIRING_RUNTIME value: &#34;1&#34; image: gcr.io/kubeflow-ci/fairing-job/fairing-job:BDE79D77 imagePullPolicy: IfNotPresent name: model resources: {} securityContext: runAsUser: 0 terminationMessagePath: /dev/termination-log terminationMessagePolicy: File workingDir: /app/ dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: conditions: - lastTransitionTime: &#34;2020-02-26T23:48:12Z&#34; lastUpdateTime: &#34;2020-02-26T23:48:12Z&#34; message: Deployment does not have minimum availability. reason: MinimumReplicasUnavailable status: &#34;False&#34; type: Available - lastTransitionTime: &#34;2020-02-26T23:48:12Z&#34; lastUpdateTime: &#34;2020-02-26T23:48:13Z&#34; message: ReplicaSet &#34;fairing-deployer-p8xc9-854c699677&#34; is progressing. reason: ReplicaSetUpdated status: &#34;True&#34; type: Progressing observedGeneration: 1 replicas: 1 unavailableReplicas: 1 updatedReplicas: 1 . Send an inference request to the prediction server . Now that you have deployed the model into your Kubernetes cluster, you can send a REST request to preform inference | The code below reads some data, sends, a prediction request and then prints out the response | . (train_X, train_y), (test_X, test_y) = read_synthetic_input() . result = util.predict_nparray(url, test_X) pprint.pprint(result.content) . (b&#39;{&#34;data&#34;:{&#34;names&#34;:[&#34;t:0&#34;,&#34;t:1&#34;],&#34;tensor&#34;:{&#34;shape&#34;:[1,2],&#34;values&#34;:[-49.2782592&#39; b&#39;7734375,-54.25324630737305]}},&#34;meta&#34;:{}} n&#39;) . Clean up the prediction endpoint . You can use kubectl to delete the Kubernetes resources for your model | If you want to delete the resources uncomment the following lines and run them | . # !kubectl delete service -l app=ames # !kubectl delete deploy -l app=ames . Track Models and Artifacts . . Using Kubeflow&#39;s metadata server you can track models and artifacts | The ModelServe code was instrumented to log executions and outputs | You can access Kubeflow&#39;s metadata UI by selecting Artifact Store from the central dashboard See here for instructions on connecting to Kubeflow&#39;s UIs | . | You can also use the python SDK to read and write entries | This notebook illustrates a bunch of metadata functionality | . Create a workspace . Kubeflow metadata uses workspaces as a logical grouping for artifacts, executions, and datasets that belong together | Earlier in the notebook we defined the function create_workspace to create a workspace for this example | You can use that function to return a workspace object and then call list to see all the artifacts in that workspace | . ws = create_workspace() ws.list() . MetadataStore with gRPC connection initialized . [{&#39;id&#39;: 3, &#39;workspace&#39;: &#39;xgboost-synthetic&#39;, &#39;run&#39;: &#39;xgboost-synthetic-faring-run2020-02-26T23:26:36.443396&#39;, &#39;version&#39;: &#39;2020-02-26T23:26:36.660862&#39;, &#39;owner&#39;: &#39;someone@kubeflow.org&#39;, &#39;description&#39;: &#39;housing price prediction model using synthetic data&#39;, &#39;name&#39;: &#39;housing-price-model&#39;, &#39;model_type&#39;: &#39;linear_regression&#39;, &#39;create_time&#39;: &#39;2020-02-26T23:26:36.660887Z&#39;, &#39;uri&#39;: &#39;mockup-model.dat&#39;, &#39;training_framework&#39;: {&#39;name&#39;: &#39;xgboost&#39;, &#39;version&#39;: &#39;0.9.0&#39;}, &#39;hyperparameters&#39;: {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 50}, &#39;labels&#39;: None, &#39;kwargs&#39;: {}}, {&#39;id&#39;: 6, &#39;workspace&#39;: &#39;xgboost-synthetic&#39;, &#39;run&#39;: &#39;xgboost-synthetic-faring-run2020-02-26T23:27:11.144500&#39;, &#39;create_time&#39;: &#39;2020-02-26T23:27:11.458520Z&#39;, &#39;version&#39;: &#39;2020-02-26T23:27:11.458480&#39;, &#39;owner&#39;: &#39;someone@kubeflow.org&#39;, &#39;description&#39;: &#39;housing price prediction model using synthetic data&#39;, &#39;name&#39;: &#39;housing-price-model&#39;, &#39;model_type&#39;: &#39;linear_regression&#39;, &#39;uri&#39;: &#39;mockup-model.dat&#39;, &#39;training_framework&#39;: {&#39;name&#39;: &#39;xgboost&#39;, &#39;version&#39;: &#39;0.9.0&#39;}, &#39;hyperparameters&#39;: {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 50}, &#39;labels&#39;: None, &#39;kwargs&#39;: {}}, {&#39;id&#39;: 9, &#39;workspace&#39;: &#39;xgboost-synthetic&#39;, &#39;run&#39;: &#39;xgboost-synthetic-faring-run2020-02-26T23:30:04.636580&#39;, &#39;create_time&#39;: &#39;2020-02-26T23:30:04.866997Z&#39;, &#39;version&#39;: &#39;2020-02-26T23:30:04.866972&#39;, &#39;owner&#39;: &#39;someone@kubeflow.org&#39;, &#39;description&#39;: &#39;housing price prediction model using synthetic data&#39;, &#39;name&#39;: &#39;housing-price-model&#39;, &#39;model_type&#39;: &#39;linear_regression&#39;, &#39;uri&#39;: &#39;mockup-model.dat&#39;, &#39;training_framework&#39;: {&#39;name&#39;: &#39;xgboost&#39;, &#39;version&#39;: &#39;0.9.0&#39;}, &#39;hyperparameters&#39;: {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 50}, &#39;labels&#39;: None, &#39;kwargs&#39;: {}}, {&#39;id&#39;: 12, &#39;workspace&#39;: &#39;xgboost-synthetic&#39;, &#39;run&#39;: &#39;xgboost-synthetic-faring-run2020-02-26T23:44:47.344352&#39;, &#39;create_time&#39;: &#39;2020-02-26T23:44:47.585805Z&#39;, &#39;version&#39;: &#39;2020-02-26T23:44:47.585782&#39;, &#39;owner&#39;: &#39;someone@kubeflow.org&#39;, &#39;description&#39;: &#39;housing price prediction model using synthetic data&#39;, &#39;name&#39;: &#39;housing-price-model&#39;, &#39;model_type&#39;: &#39;linear_regression&#39;, &#39;uri&#39;: &#39;mockup-model.dat&#39;, &#39;training_framework&#39;: {&#39;name&#39;: &#39;xgboost&#39;, &#39;version&#39;: &#39;0.9.0&#39;}, &#39;hyperparameters&#39;: {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 50}, &#39;labels&#39;: None, &#39;kwargs&#39;: {}}, {&#39;id&#39;: 15, &#39;workspace&#39;: &#39;xgboost-synthetic&#39;, &#39;run&#39;: &#39;xgboost-synthetic-faring-run2020-02-26T23:48:06.287002&#39;, &#39;version&#39;: &#39;2020-02-26T23:48:06.495138&#39;, &#39;owner&#39;: &#39;someone@kubeflow.org&#39;, &#39;description&#39;: &#39;housing price prediction model using synthetic data&#39;, &#39;name&#39;: &#39;housing-price-model&#39;, &#39;model_type&#39;: &#39;linear_regression&#39;, &#39;create_time&#39;: &#39;2020-02-26T23:48:06.495166Z&#39;, &#39;uri&#39;: &#39;mockup-model.dat&#39;, &#39;training_framework&#39;: {&#39;name&#39;: &#39;xgboost&#39;, &#39;version&#39;: &#39;0.9.0&#39;}, &#39;hyperparameters&#39;: {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 50}, &#39;labels&#39;: None, &#39;kwargs&#39;: {}}] . Create a pipeline to train your model . Kubeflow pipelines makes it easy to define complex workflows to build and deploy models | Below you will define and run a simple one step pipeline to train your model | Kubeflow pipelines uses experiments to group different runs of a pipeline together | So you start by defining a name for your experiement | . Define the pipeline . To create a pipeline you create a function and decorate it with the @dsl.pipeline decorator . You use the decorator to give the pipeline a name and description | . | Inside the function, each step in the function is defined by a ContainerOp that specifies a container to invoke . | You will use the container image that you built earlier using Kubeflow Fairing . | Since the Kubeflow Fairing preprocessor added a main function using python-fire, a step in your pipeline can invocation any function in the ModelServe class just by setting the command for the container op | See the pipelines SDK reference for more information | . @dsl.pipeline( name=&#39;Training pipeline&#39;, description=&#39;A pipeline that trains an xgboost model for the Ames dataset.&#39; ) def train_pipeline( ): command=[&quot;python&quot;, preprocessor.executable.name, &quot;train&quot;] train_op = dsl.ContainerOp( name=&quot;train&quot;, image=builder.image_tag, command=command, ).apply( gcp.use_gcp_secret(&#39;user-gcp-sa&#39;), ) train_op.container.working_dir = &quot;/app&quot; . Compile the pipeline . Pipelines need to be compiled | . pipeline_func = train_pipeline pipeline_filename = pipeline_func.__name__ + &#39;.pipeline.zip&#39; compiler.Compiler().compile(pipeline_func, pipeline_filename) . Submit the pipeline for execution . Pipelines groups runs using experiments | So before you submit a pipeline you need to create an experiment or pick an existing experiment | Once you have compiled a pipeline, you can use the pipelines SDK to submit that pipeline | . EXPERIMENT_NAME = &#39;MockupModel&#39; #Specify pipeline argument values arguments = {} # Get or create an experiment and submit a pipeline run client = kfp.Client() experiment = client.create_experiment(EXPERIMENT_NAME) #Submit a pipeline run run_name = pipeline_func.__name__ + &#39; run&#39; run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments) #vvvvvvvvv This link leads to the run information page. (Note: There is a bug in JupyterLab that modifies the URL and makes the link stop working) . Creating experiment MockupModel. . Experiment link here Run link here",
            "url": "https://blog.kubeflow.org/jupyter/2020/07/27/lineage-tracking.html",
            "relUrl": "/jupyter/2020/07/27/lineage-tracking.html",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Kubeflow & Kale simplify building better ML Pipelines with automatic hyperparameter tuning",
            "content": "Running pipelines at scale has never been easier. . Kubeflow’s Kale is maturing and fast becoming the superfood that glues together the main Kubeflow components to provide a cohesive and seamless data science experience. . TL;DR: Convert Notebook to Kubeflow Pipelines, run them as hyperparameter tuning experiments, track executions and artifacts with MLMD, cache and maintain an immutable history of executions: Kale brings all of this on the table in a unified workflow tool, simple to use. . Running pipelines at scale has never been easier . Kubeflow’s Kale is maturing and fast becoming the superfood that glues together the main Kubeflow components to provide a cohesive and seamless data science experience. With its newest release, Kale provides an end-to-end workflow that encompasses Jupyter Notebooks, Kubeflow Pipelines, hyperparameter tuning with Katib, metadata tracking with ML Metadata (MLMD), and faster pipeline executions with caching. . If you are new to Kale, head over to this short introduction to get started! . In this blog post, you will learn about the features that Kale is bringing to the Machine Learning community with version 0.5, and learn how to get started with a curated example. . New Face . First off, we are excited to reveal the new Kale logo. Kudos to Konstantinos Palaiologos (Arrikto) for designing the brand new, modern Kale leaf. This will be the new face of the project from now on. . . Hyperparameter Tuning . The major new addition in v0.5 is the support for running pipelines with Katib. Katib is Kubeflow’s component to run general purpose hyperparameter tuning jobs. Just as you would press a single button to convert a notebook to a pipeline, you can now press a button and let Kale start a hyperparameter Job on that pipeline. All you need to do is tell Kale what the HP tuning job should search for. . Running hyperparameter tuning jobs gives you a dramatic boost in delivering good results for your project. The *manual *tuning process of running your model countless times, using different parameters combinations, aggregating them and comparing them, is error-prone and inefficient. Delegating this work to an automated process allows you to become faster, more efficient and accurate. . Parametrize the HP tuning Job directly from the notebook . Katib does not know anything about the jobs that it is actually running (called Trials in the Katib jargon). Katib supports running Trials as simple Jobs (that is, Pods), BatchJobs, TFJobs, and PyTorchJobs. Kale 0.5 integrates Katib with Kubeflow Pipelines. This enables Katib trails to run as pipelines in KFP. The metrics from the pipeline runs are provided to help in model performance analysis and debugging. All Kale needs to know from the user is the search space, the optimization algorithm, and the search goal. . Kale will also make sure that all the runs of a Katib experiment, end up unified and grouped, under a single KFP experiment, to make it easy to search and isolate a particular job. . Kale will also show a live view of the running experiments, directly in the notebook, so you will know how many pipelines are still running and, upon completion, which one performed best. . New features . Pipeline parameters and metrics . In order to run pipelines with hyperparameter tuning, the pipeline needs to be able to accept arguments and produce metrics. Enabling the pipeline to do this, is now tremendously easy. Kale provides two new cell tags: pipeline-parameters and pipeline-metrics. . . Assigning the pipeline-parameters tag on any cell that contains some variables will instruct Kale to transform them to pipeline parameters. These values will then be passed to the pipeline steps that actually make use of them. . . If you want the pipeline to produce some metrics, just print them at the end of the notebook and assign the pipeline-metrics tag to the cell. Kale will take care of understanding which steps produce the metrics and you will see them appear in the KFP dashboard. . Rich notebook outputs . Having your pipelines produce rich outputs (like plots, tables, metrics, …) that can be captured and displayed by the Kubeflow Pipelines dashboard has always been somewhat cumbersome. You would need to write some KFP-specific code to produce json artifacts that would then be interpreted by KFP. . What if you could just write plain Python in your Notebook using your favourite plotting library, and have the plots auto-magically appear as KFP outputs, when the Notebook gets compiled into a pipeline? . Now, when running your notebook code inside a pipeline step, Kale will wrap it and feed it to an ipython kernel, so that all the nice artifacts produced in the notebook, will be produced in the pipeline as well. Kale will capture all these rich outputs automatically and instruct KFP to display them in the dashboard. Effectively, whatever happens in the notebook, now happens in the pipeline as well. The execution context is exactly the same. . Any rich output that is visible in the notebook gets captured by Kale and exposed in the KFP dashboard. . MLMD Integration . An important part of running reproducible Machine Learning collaboratively and at scale, is being able to track pipeline executions, their inputs, their outputs and how these are connected together. Kubeflow provides an ML Metadata service which serves this exact purpose. This service also includes a lineage view to enable the user to have a deep insight into the whole history of events. . Kale is now fully integrated with this service, logging each new execution automatically alongside all the artifacts produced by the pipeline. . Run on GPU . If you need to run a particular step on a GPU node, Kale has you covered too. You can now annotate steps with a dedicated dialog, directly from the notebook and each step can have its own annotations. This is just the first iteration, Kale will support adding any kind of K8s limits or annotations to pipeline steps in the near future. . . Overall UI and performance improvements . The new version of Kale’s JupyterLab brings tons of performance improvements and UI enhancements. Updating the notebook cells’ annotations will now be easier and faster. We covered many corner cases and solved tons of bugs. The UI of the cell’s annotation editor is more consistent with the overall Jupyter style and much more intuitive and easy to use. A big shoutout to Tasos Alexiou (Arrikto) for having spent countless hours in understanding the Jupyter internals and improving our application lifecycle. . Hands-On . To start playing-around with Kale v0.5, head over to the GitHub repository and follow the installation instructions. If you are already running Kubeflow (either in your own cluster or on MiniKF), spin up a new Notebook Server using the image gcr.io/arrikto/jupyter-kale:v0.5.0. . Note: Kale v0.5 needs to run on Kubeflow ≥ 1.0. Also, make sure that the following Kubeflow components are updated as follows: . Katib controller: gcr.io/arrikto/katib-controller:40b5b51a . | Katib Chocolate service: gcr.io/arrikto/suggestion-chocolate:40b5b51a . | . We will release a new version of MiniKF very soon, containing a lot of improvements that will make the Kale experience even better. You will also be able to go through a new Codelab to try out the Kale-Katib integration yourself. Stay tuned for updates on the Arrikto channels. . Road Ahead . We are always looking to improve Kale and help data scientists have a seamless ML workflow from writing code to training, optimizing, and serving their models. . We are excited to have the ML community try out this new version of Kale and the coming MiniKF update. . A special mention must go to the various members of the Arrikto team (Ilias Katsakioris, Chris Pavlou, Kostis Lolos, Tasos Alexiou) who contributed to delivering all these new features. .",
            "url": "https://blog.kubeflow.org/integrations/2020/07/10/kubeflow-kale.html",
            "relUrl": "/integrations/2020/07/10/kubeflow-kale.html",
            "date": " • Jul 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Introduction to Kubeflow MPI Operator and Industry Adoption",
            "content": "Kubeflow just announced its first major 1.0 release recently, which makes it easy for machine learning engineers and data scientists to leverage cloud assets (public or on-premise) for machine learning workloads. In this post, we’d like to introduce MPI Operator (docs), one of the core components of Kubeflow, currently in alpha, which makes it easy to run synchronized, allreduce-style distributed training on Kubernetes. . There are two major distributed training strategies nowadays: one based on parameter servers and the other based on collective communication primitives such as allreduce. . Parameter server based distribution strategy relies on centralized parameter servers for coordination between workers, responsible for collecting gradients from workers and sending updated parameters to workers. The diagram below shows the interaction between parameter servers and worker nodes under this distributed training strategy. . . While distributed training based on parameter servers can support training very large models and datasets by adding more workers and parameter servers, there are additional challenges involved in order to optimize the performance: . It is not easy to identify the right ratio of the number of workers to the number of parameter servers. For example, if only a small number of parameter servers are used, network communication will likely become the bottleneck for training. . | If many parameter servers are used, the communication may saturate network interconnects. . | The memory quota of workers and parameter servers requires fine tuning to avoid out-of-memory errors or memory waste. . | If the model could fit within the computational resources of each worker, additional maintenance and communication overheads are introduced when the model is partitioned to multiple parameter servers. . | We need to replicate the model on each parameter server in order to support fault-tolerance, which requires additional computational and storage resources. . | . In contrast, distributed training based on collective communication primitives such as allreduce could be more efficient and easier to use in certain use cases. Under allreduce-based distributed training strategy, each worker stores a complete set of model parameters. In other words, no parameter server is needed. Allreduce-based distributed training could address many of the challenges mentioned above: . Each worker stores a complete set of model parameters, no parameter server is needed, so it’s straightforward to add more workers when necessary. . | Failures among the workers can be recovered easily by restarting the failed workers and then load the current model from any of the existing workers. Model does not need to be replicated to support fault-tolerance. . | The model can be updated more efficiently by fully leveraging the network structure and collective communication algorithms. For example, in ring-allreduce algorithm, each of the N workers only needs to communicate with two of its peer workers 2 * (N − 1) times to update all the model parameters completely. . | Scaling up and down the number of workers is as easy as reconstructing the underlying allreduce communicator and re-assigning the ranks among the workers. . | . There are many existing technologies available that provide implementations for these collective communication primitives such as NCCL, Gloo, and many different implementations of MPI. . MPI Operator provides a common Custom Resource Definition (CRD) for defining a training job on a single CPU/GPU, multiple CPU/GPUs, and multiple nodes. It also implements a custom controller to manage the CRD, create dependent resources, and reconcile the desired states. . . Unlike other operators in Kubeflow such as TF Operator and PyTorch Operator that only supports for one machine learning framework, MPI operator is decoupled from underlying framework so it can work well with many frameworks such as Horovod, TensorFlow, PyTorch, Apache MXNet, and various collective communication implementations such as OpenMPI. . For more details on comparisons between different distributed training strategies, various Kubeflow operators, please check out our presentation at KubeCon Europe 2019. . Example API Spec . We’ve been working closely with the community and industry adopters to improve the API spec for MPI Operator so it’s suitable for many different use cases. Below is an example: . apiVersion: kubeflow.org/v1alpha2 kind: MPIJob metadata: name: tensorflow-benchmarks spec: slotsPerWorker: 1 cleanPodPolicy: Running mpiReplicaSpecs: Launcher: replicas: 1 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks command: - mpirun - python - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py - --model=resnet101 - --batch_size=64 - --variable_update=horovod Worker: replicas: 2 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks resources: limits: nvidia.com/gpu: 1 . Note that MPI Operator provides a flexible but user-friendly API that’s consistent across other Kubeflow operators. . Users can easily customize their launcher and worker pod specs by modifying the relevant sections in the template. For example, customizing to use various types of computational resources such as CPUs, GPUs, memory, etc. . In addition, below is an example spec that performs distributed TensorFlow training job using ImageNet data in TFRecords format stored in a Kubernetes volume: . apiVersion: kubeflow.org/v1alpha2 kind: MPIJob metadata: name: tensorflow-benchmarks spec: slotsPerWorker: 1 cleanPodPolicy: Running mpiReplicaSpecs: Launcher: replicas: 1 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks command: - mpirun - python - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py - --model=resnet101 - --batch_size=64 - --variable_update=horovod Worker: replicas: 2 template: spec: containers: - image: mpioperator/tensorflow-benchmarks:latest name: tensorflow-benchmarks resources: limits: nvidia.com/gpu: 1 . Architecture . MPI Operator contains a custom controller that listens for changes in MPIJob resources. When a new MPIJob is created, the controller goes through the following logical steps: . Create a ConfigMap that contains: | A helper shell script that can be used by mpirun in place of ssh. It invokes kubectl exec for remote execution. . | A hostfile that lists the pods in the worker StatefulSet (in the form of ${job-id}-worker-0, ${job-id}-worker-1, …), and the available slots (CPUs/GPUs) in each pod. . | . Create the RBAC resources (Role, ServiceAccount, RoleBinding) to allow remote execution (pods/exec). . | Wait for the worker pods to be ready. . | Create the launcher job. It runs under the ServiceAccount created in step 2, and sets up the necessary environment variables for executing mpirun commands remotely. The kubectl binary is delivered to an emptyDir volume through an init container. . | After the launcher job finishes, set the replicas to 0 in the worker StatefulSet. . | . For more details, please check out the design doc for MPI Operator. . Industry Adoption . At the time of writing, there are 13 disclosed industry adopters and many others who’ve been working closely with the community to reach where we are today. We’d like to showcase some of the use cases of MPI Operator in several companies. If your company would like to be included in the list of adopters, please send us a pull request on GitHub! . Ant Financial . At Ant Financial, we manage Kubernetes clusters with tens of thousands of nodes and have deployed the MPI Operator along with other Kubeflow operators. The MPI Operator leverages the network structure and collective communication algorithms so that users don’t have to worry about the right ratio between the number of workers and parameter servers to obtain the best performance. Users can focus on building out their model architectures without spending time on tuning the downstream infrastructure for distributed training. . The models produced have been widely deployed in production and battle-tested in many different real life scenarios. One notable use case is Saofu — a mobile app for users to scan any “福” (Chinese character that represents fortune) through augmented reality to enter a lucky draw where each user would receive a virtual red envelope with a portion of a significant amount of money. . Bloomberg . Bloomberg, the global business and financial information and news leader, possesses an enormous amount of data — from historical news to real-time market data and everything in between. Bloomberg’s Data Science Platform was built to allow the company’s internal machine learning engineers and data scientists to more easily leverage data and algorithmic models in their daily work, including when training jobs and automatic machine learning models used in the state-of-the-art solutions they’re building. . “The Data Science Platform at Bloomberg offers a TensorFlowJob CRD similar to Kubeflow’s own TFJob, enabling the company’s data scientists to easily train neural network models. Recently, the Data Science Platform team enabled Horovod-based distributed training in its TensorFlowJob via the MPI Operator as an implementation detail. Using MPIJob in the back-end has allowed the Bloomberg Data Science Platform team to quickly offer its machine learning engineers a robust way to train a BERT model within hours using the company’s large corpus of text data’’, says Chengjian Zheng, software engineer from Bloomberg. . Caicloud . Caicloud Clever is an artificial intelligence cloud platform based on Caicloud container cloud platform with powerful hardware resource management and efficient model development capabilities. Caicloud products have been deployed in many 500 China Fortune companies. . “Caicloud Clever supports multiple frameworks of AI model training including TensorFlow, Apache MXNet, Caffe, PyTorch with the help of Kubeflow tf-operator, pytorch-operator and others”, says Ce Gao, AI infrastructure engineer from Caicloud Clever team. “While RingAllReduce distributed training support is requested for improved customer maturity.” . Kubeflow MPI operator is a Kubernetes Operator for allreduce-style distributed training. Caicloud Clever team adopts MPI Operator’s v1alpha2 API. The Kubernetes native API makes it easy to work with the existing systems in the platform. . Iguazio . Iguazio provides a cloud-native data science platform with emphasis on automation, performance, scalability, and use of open-source tools. . According to Yaron Haviv, the Founder and CTO of Iguazio, “We evaluated various mechanisms which will allow us to scale deep learning frameworks with minimal developer effort and found that using the combination of Horovod with the MPI Operator over Kubernetes is the best tool for the job since it enable horizontal scalability, supports multiple frameworks such as TensorFlow and PyTorch and doesn’t require too much extra coding or the complex use of parameter servers.” . Iguazio have integrated the MPI Operator into its managed service offering and its fast data layer for maximum scalability, and work to simplify the usage through open source projects like MLRun (for ML automation and tracking). Check out this blog post with an example application that demonstrates Iguazio’s usage of the MPI Operator. . Polyaxon . Polyaxon is a platform for reproducible and scalable machine learning on Kubernetes, it allows users to iterate faster on their research and model creation. Polyaxon provides a simple abstraction for data scientists and machine learning engineers to streamline their experimentation workflow, and provides a very cohesive abstraction for training and tracking models using popular frameworks such as Scikit-learn, TensorFlow, PyTorch, Apache MXNet, Caffe, etc. . “Several Polyaxon users and customers were requesting an easy way to perform an allreduce-style distributed training, the MPI Operator was the perfect solution to provide such abstraction. Polyaxon is deployed at several companies and research institutions, and the public docker hub has over 9 million downloads.”, says Mourad Mourafiq, the Co-founder of Polyxagon. . Community and Call for Contributions . We are grateful for over 28 individual contributors from over 11 organizations, namely Alibaba Cloud, Amazon Web Services, Ant Financial, Bloomberg, Caicloud, Google Cloud, Huawei, Iguazio, NVIDIA, Polyaxon, and Tencent, that have contributed directly to MPI Operator’s codebase and many others who’ve filed issues or helped resolve them, asked and answered questions, and were part of inspiring discussions. We’ve put together a roadmap that provides a high-level overview of where the MPI Operator will grow in future releases and we welcome any contributions from the community! . We could not have achieved our milestones without an incredibly active community. Check out our community page to learn more about how to join the Kubeflow community! . Originally published at https://terrytangyuan.github.io on March 17, 2020. .",
            "url": "https://blog.kubeflow.org/integrations/operators/2020/03/16/mpi-operator.html",
            "relUrl": "/integrations/operators/2020/03/16/mpi-operator.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Kubeflow 1.0 - Cloud Native ML for Everyone",
            "content": "Kubeflow 1.0: Cloud Native ML for Everyone . On behalf of the entire community, we are proud to announce Kubeflow 1.0, our first major release. Kubeflow was open sourced at Kubecon USA in December 2017, and during the last two years the Kubeflow Project has grown beyond our wildest expectations. There are now hundreds of contributors from over 30 participating organizations. . Kubeflow’s goal is to make it easy for machine learning (ML) engineers and data scientists to leverage cloud assets (public or on-premise) for ML workloads. You can use Kubeflow on any Kubernetes-conformant cluster. . With 1.0, we are graduating a core set of stable applications needed to develop, build, train, and deploy models on Kubernetes efficiently. (Read more in Kubeflow’s versioning policies and application requirements for graduation.) . Graduating applications include: . Kubeflow’s UI, the central dashboard . | Jupyter notebook controller and web app . | Tensorflow Operator (TFJob) and PyTorch Operator for distributed training . | kfctl for deployment and upgrades . | Profile controller and UI for multiuser management . | . Hear more about Kubeflow’s mission and 1.0 release in this interview with Kubeflow founder and core contributor Jeremy Lewi on the Kubernetes Podcast. . Develop, Build, Train, and Deploy with Kubeflow . Kubeflow’s 1.0 applications that make up our develop, build, train, deploy critical user journey. . With Kubeflow 1.0, users can use Jupyter to develop models. They can then use Kubeflow tools like fairing (Kubeflow’s python SDK) to build containers and create Kubernetes resources to train their models. Once they have a model, they can use KFServing to create and deploy a server for inference. . Getting Started with ML on Kubernetes . Kubernetes is an amazing platform for leveraging infrastructure (whether on public cloud or on-premises), but deploying Kubernetes optimized for ML and integrated with your cloud is no easy task. With 1.0 we are providing a CLI and configuration files so you can deploy Kubeflow with one command: . kfctl apply -f [kfctl_gcp_iap.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_k8s_istio.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_aws_cognito.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) kfctl apply -f [kfctl_ibm.v1.0.0.yaml](https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_gcp_iap.yaml) . Jupyter on Kubernetes . In Kubeflow’s user surveys, data scientists have consistently expressed the importance of Jupyter notebooks. Further, they need the ability to integrate isolated Jupyter notebooks with the efficiencies of Kubernetes on Cloud to train larger models using GPUs and run multiple experiments in parallel. Kubeflow makes it easy to leverage Kubernetes for resource management and put the full power of your datacenter at the fingertips of your data scientist. . With Kubeflow, each data scientist or team can be given their own namespace in which to run their workloads. Namespaces provide security and resource isolation. Using Kubernetes resource quotas, platform administrators can easily limit how much resources an individual or team can consume to ensure fair scheduling. . After deploying Kubeflow, users can leverage Kubeflow’s central dashboard for launching notebooks: . Kubeflow’s UI for managing notebooks: view and connect to existing notebooks or launch a new one. . In the Kubeflow UI users can easily launch new notebooks by choosing one of the pre-built docker images for Jupyter or entering the URL of a custom image. Next, users can set how many CPUs and GPUs to attach to their notebook. Notebooks can also include configuration and secrets parameters which simplify access to external repositories and databases. . . Training faster with distributed training . Distributed training is the norm at Google (blog), and one of the most exciting and requested features for deep learning frameworks like TensorFlow and PyTorch. . When we started Kubeflow, one of our key motivations was to leverage Kubernetes to simplify distributed training. Kubeflow provides Kubernetes custom resources that make distributed training with TensorFlow and PyTorch simple. All a user needs to do is define a TFJob or PyTorch resource like the one illustrated below. The custom controller takes care of spinning up and managing all of the individual processes and configuring them to talk to one another: . apiVersion: kubeflow.org/v1 kind: TFJob metadata: name: mnist-train spec: tfReplicaSpecs: Chief: replicas: 1 spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow Ps: replicas: 1 template: spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow Worker: replicas: 10 spec: containers: image: gcr.io/alice-dev/fairing-job/mnist name: tensorflow . Monitoring Model Training With TensorBoard . To train high quality models, data scientists need to debug and monitor the training process with tools like Tensorboard. With Kubernetes and Kubeflow, userscan easily deploy TensorBoard on their Kubernetes cluster by creating YAML files like the ones below. When deploying TensorBoard on Kubeflow, users can take advantage of Kubeflow’s AuthN and AuthZ integration to securely access TensorBoard behind Kubeflow’s ingress on public clouds: . // On GCP: [https://${KFNAME}.endpoints.${PROJECT}.cloud.goog/mnist/kubeflow-mnist/tensorboard/](https://${KFNAME}.endpoints.${PROJECT}.cloud.goog/mnist/kubeflow-mnist/tensorboard/) // On AWS: [http://8fb34ebe-istiosystem-istio-2af2-925939634.us-west-2.elb.amazonaws.com/mnist/anonymous/tensorboard/](http://8fb34ebe-istiosystem-istio-2af2-925939634.us-west-2.elb.amazonaws.com/mnist/anonymous/tensorboard/) . No need to kubectl port-forward to individual pods. . Deploying Models . KFServing is a custom resource built on top of Knative for deploying and managing ML models. KFServing offers the following capabilities not provided by lower level primitives (e.g. Deployment): . Deploy your model using out-of-the-box model servers (no need to write your own flask app) . | Auto-scaling based on load, even for models served on GPUs . | Safe, controlled model rollout . | Explainability (alpha) . | Payload logging (alpha) . | Below is an example of a KFServing spec showing how a model can be deployed. All a user has to do is provide the URI of their model file using storageUri: . apiVersion: &quot;serving.kubeflow.org/v1alpha2&quot; kind: &quot;InferenceService&quot; metadata: name: &quot;sklearn-iris&quot; spec: default: predictor: sklearn: storageUri: &quot;gs://kfserving-samples/models/sklearn/iris&quot; . Check out the samples to learn how to use the above capabilities. . Solutions are More Than Models . A model gathering dust in object storage isn’t doing your organization any good. To put ML to work, you typically need to incorporate that model into an application – whether it’s a web application, mobile app, or part of some backend reporting pipeline. . Frameworks like flask and bootstrap make it easy for data scientists to create rich, visually appealing web applications that put their models to work. Below is a screenshot of the UI we built for Kubeflow’s mnist example. . With Kubeflow, there is no need for data scientists to learn new concepts or platforms to deploy their applications, or to deal with ingress, networking certificates, etc. They can deploy their application just like TensorBoard; the only thing that changes is the Docker image and flags. . . If this sounds like just what you are looking for we recommend: . Visiting our docs to learn how to deploy Kubeflow on your public or private cloud. . | Walking through the mnist tutorial to try our core applications yourself. . | What’s coming in Kubeflow . There’s much more to Kubeflow than what we’ve covered in this blog post. In addition to the applications listed here, we have a number of applications under development: . Pipelines (beta) for defining complex ML workflows . | Metadata (beta) for tracking datasets, jobs, and models, . | Katib (beta) for hyper-parameter tuning . | Distributed operators for other frameworks like xgboost . | . In future releases we will be graduating these applications to 1.0. . User testimonials . All this would be nothing without feedback from and collaboration with our users. Some feedback from people using Kubeflow in production include: . *“The Kubeflow 1.0 release is a significant milestone as it positions Kubeflow to be a viable ML Enterprise platform. Kubeflow 1.0 delivers material productivity enhancements for ML researchers.” — *Jeff Fogarty, AVP ML / Cloud Engineer, US Bank . *“Kubeflow’s data and model storage allows for smooth integration into CI/CD processes, allowing for a much faster and more agile delivery of machine learning models into applications.” — *Laura Schornack, **Shared Services Architect, Chase Commercial Bank** . *“With the launch of Kubeflow 1.0 we now have a feature complete end-to-end open source machine learning platform, allowing everyone from small teams to large unicorns like Gojek to run ML at scale.” — *Willem Pienaar, Engineering Lead, Data Science Platform, GoJek . *“Kubeflow provides a seamless interface to a great set of tools that together manages the complexity of ML workflows and encourages best practices. The Data Science and Machine Learning teams at Volvo Cars are able to iterate and deliver reproducible, production grade services with ease.”— *Leonard Aukea, Volvo Cars . “With Kubeflow at the heart of our ML platform, our small company has been able to stack models in production to improve CR, find new customers, and present the right product to the right customer at the right time.” *— *Senior Director, One Technologies . “Kubeflow is helping GroupBy in standardizing ML workflows and simplifying very complicated deployments!” *— *Mohamed Elsaied, Machine Learning Team Lead, GroupBy . Thank You! . None of this would have been possible without the tens of organizations and hundreds of individuals that have been developing, testing, and evangelizing Kubeflow. . . An Open Community . We could not have achieved our milestone without an incredibly active community. Please come aboard! . Join the Kubeflow Slack channel . | Join the kubeflow-discuss mailing list . | Attend a weekly community meeting . | If you have questions, run into issues, please leverage the Slack channel and/or submit bugs via Kubeflow on GitHub. . | . Thank you all so much — onward! .",
            "url": "https://blog.kubeflow.org/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html",
            "relUrl": "/releases/2020/03/02/kubeflow-1-0-cloud-native-ml-for-everyone.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://blog.kubeflow.org/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.kubeflow.org/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This blog is hosted on GitHub Pages, via the kubeflow/blog repo. Instructions on contributing to this blog can be found there. . This website is powered by fastpages. .",
          "url": "https://blog.kubeflow.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.kubeflow.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}